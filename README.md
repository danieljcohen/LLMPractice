# Building an LLM from Scratch

Welcome to the GitHub repository for my project on building a Large Language Model (LLM) from scratch, following Andrej Karpathy's comprehensive course. This repository includes the code, model training steps, and relevant resources used throughout the project.


## Introduction

This project is a hands-on implementation of a Large Language Model (LLM) from scratch. The model is built using principles from neural networks and deep learning, inspired by the methodologies in Karpathy's "Neural Networks: Zero to Hero" course. The main goal is to understand and implement the core components of LLMs, including data preprocessing, model architecture, training, and evaluation.


## Setup and Installation

To get started with this project, clone the repository and install the necessary dependencies:

- Pytorch: https://pytorch.org/get-started/locally/


## Model Architecture

The model architecture is based on the Transformer model, specifically a Generatively Pretrained Transformer (GPT). The implementation can be found the jupyter notebook file, in which the process for creating this model is broken down step by step.


## Usage

Run bigram.py with one of the given input files provided.
